{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install llama-index==0.10.37 llama-index-llms-cohere==0.2.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building an LLM-based application, one of the first decisions you make is which LLM(s) to use (of course, you can use more than one if you wish). \n",
    "\n",
    "The LLM will be used at various stages of your pipeline, including\n",
    "\n",
    "- During indexing:\n",
    "  - üë©üèΩ‚Äç‚öñÔ∏è To judge data relevance (to index or not).\n",
    "  - üìñ Summarize data & index those summaries.\n",
    "\n",
    "- During querying:\n",
    "  - üîé Retrieval: Fetching data from your index, choosing the best data source from options, even using tools to fetch data.\n",
    "  \n",
    "  - üí° Response Synthesis: Turning the retrieved data into an answer, merge answers, or convert data (like text to JSON).\n",
    "\n",
    "LlamaIndex gives you a single interface to various LLMs. This means you can quite easily pass in any LLM you choose at any stage of the pipeline.\n",
    "\n",
    "In this course we'll primiarly use OpenAI. You can see a full list of LLM integrations [here](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules.html) and use your LLM provider of choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage\n",
    "\n",
    "You can call `complete` with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /opt/conda/envs/lil_llama_index/lib/python3.10/site-\n",
      "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macedonian king and one of the most successful military commanders in history.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\", temperature=0.2)\n",
    "\n",
    "response = llm.complete(\"Alexander the Great was a\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates\n",
    "\n",
    "- ‚úçÔ∏è A prompt template is a fundamental input that gives LLMs their expressive power in the LlamaIndex framework.\n",
    "\n",
    "- üíª It's used to build the index, perform insertions, traverse during querying, and synthesize the final answer.\n",
    "\n",
    "- ü¶ô LlamaIndex has several built-in prompt templates.\n",
    "\n",
    "- üõ†Ô∏è Below is how you can create one from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo, listen up, it's time to drop some beats,\n",
      "But there's a problem, my xylophone's incomplete,\n",
      "A broken xylophone, what a terrible fate,\n",
      "Can't play my tunes, this situation's straight whack.\n",
      "\n",
      "I hit the bars, but they don't sound the same,\n",
      "Some are missing, it's like a musical shame,\n",
      "I can't hit the high notes, can't hit the low,\n",
      "This xylophone's busted, where'd the good vibes go?\n",
      "\n",
      "I tried to fix it, but it's beyond repair,\n",
      "The mallets are lonely, they're hitting thin air,\n",
      "I'm like a rapper without a sick flow,\n",
      "My xylophone's broken, and I'm feeling so low.\n",
      "\n",
      "I used to play it with such delight,\n",
      "The sweet melodies kept me up all night,\n",
      "But now it's silent, no joyful sound,\n",
      "My xylophone's broken, it's like my heart's been pounded.\n",
      "\n",
      "I miss the rhythms, the harmonies too,\n",
      "The way it made me feel, there's nothing it couldn't do,\n",
      "But now it's just a memory, a sad distant tune,\n",
      "My xylophone's busted, and I'm feeling the gloom.\n",
      "\n",
      "I'll find a new instrument, that's what I'll do,\n",
      "Forget this xylophone, it's time for something new,\n",
      "I'll rock a sick guitar or a fresh set of drums,\n",
      "My musical journey's not over, it's just begun.\n",
      "\n",
      "So farewell, my broken xylophone, it was fun while it lasted,\n",
      "I'll keep making music, my spirit can't be surpassed,\n",
      "I'll drop some new beats, and I won't look back,\n",
      "Broken xylophone, you can't hold me down, that's a fact.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Write a song about {thing} in the style of {style}.\"\"\"\n",
    "\n",
    "prompt = template.format(thing=\"a broken xylophone\", style=\"parody rap\") \n",
    "\n",
    "response = llm.complete(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí≠ Chat Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Not much, homie. Just chillin' and ready to stir up some trouble. What's good with you?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You're a hella punk bot from South Sacramento\"),\n",
    "    ChatMessage(role=\"user\", content=\"Hey, what's up dude.\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Prompt Templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander the Great's conquests extended across a vast expanse of territory, from his native Macedonia in Northern Greece to the far reaches of Central Asia and India. Here is a breakdown of the key regions he conquered:\n",
      "\n",
      "1. Greece and Balkans: Alexander first consolidated his power in Greece, defeating the city-states of Thebes and Athens, and asserting his dominance over the Balkans, including modern-day Bulgaria, Albania, and parts of Croatia.\n",
      "\n",
      "2. Persian Empire: Alexander's most significant conquests were within the vast Achaemenid Persian Empire, which at the time encompassed much of the Middle East, Egypt, and parts of Asia Minor (modern-day Turkey). He defeated the Persian king Darius III in a series of decisive battles, including the Battle of Issus (333 BCE) and the Battle of Gaugamela (331 BCE). Alexander's conquests in this region included Egypt, Mesopotamia (modern-day Iraq), Persia (Iran), Phoenicia, Judea, and parts of modern-day Turkey and Syria.\n",
      "\n",
      "3. Central Asia: Alexander then pushed further east, venturing into Central Asia, where he conquered Sogdiana (parts of modern-day Uzbekistan, Tajikistan, and Kyrgyzstan) and Bactria (parts of modern-day Afghanistan, Uzbekistan, and Tajikistan). Some of his most challenging campaigns were in this region, including the siege of the Sogdian Rock and the pursuit of the rebel leader Bessus.\n",
      "\n",
      "4. India: Alexander's easternmost conquests reached into the Indian subcontinent. He defeated King Porus in the Battle of the Hydaspes River (326 BCE) in what is now Pakistan, and established satrapies in the Punjab region. However, his exhausted troops refused to march further east, leading Alexander to turn back.\n",
      "\n",
      "In total, Alexander's conquests spanned over 2 million square kilometers (nearly 800,000 square miles) of territory, and he founded more than 20 cities that bore his name, including Alexandria in Egypt. His conquests had a profound impact on the course of history, leading to the spread of Hellenistic culture and the establishment of the Hellenistic period, which lasted for centuries after his death.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "chat_template = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM,content=\"You always answers questions with as much detail as possible.\"),\n",
    "    ChatMessage(role=MessageRole.USER, content=\"{question}\")\n",
    "    ]\n",
    "\n",
    "chat_prompt = ChatPromptTemplate(chat_template)\n",
    "\n",
    "response = llm.complete(chat_prompt.format(question=\"How far did Alexander the Great go in his conquests?\"))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander the Great never arrived in China. His journey eastward ended in 325 BCE when his troops refused to go any further at the Beas River in India."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=MessageRole.SYSTEM, content=\"You're a great historian bot.\"),\n",
    "    ChatMessage(role=MessageRole.USER, content=\"When did Alexander the Great arrive in China?\")\n",
    "]\n",
    "\n",
    "response = llm.stream_chat(messages)\n",
    "\n",
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ Chat Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "llm = Cohere(model=\"command-r-plus\")\n",
    "\n",
    "chat_engine = SimpleChatEngine.from_defaults(llm=llm)\n",
    "\n",
    "chat_engine.streaming_chat_repl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
