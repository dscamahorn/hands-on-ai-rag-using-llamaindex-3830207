{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install llama-index mistralai llama-index-embeddings-mistralai qdrant-client llama-index-vector-stores-qdrant llama-index-llms-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_API_KEY = os.environ['MISTRAL_API_KEY']\n",
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")\n",
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /opt/conda/envs/llama/lib/python3.13/site-\n",
      "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "\n",
    "\n",
    "Settings.llm = MistralAI(\n",
    "    model=\"mistral-small-latest\", \n",
    "    api_key=MISTRAL_API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "Settings.embed_model = MistralAIEmbedding(\n",
    "    model_name=\"mistral-embed\", \n",
    "    api_key=MISTRAL_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap of the LlamaIndex Order of Operations\n",
    "\n",
    "In LlamaIndex, the order of operations in the query pipeline typically follows these steps:\n",
    "\n",
    "**üçΩÔ∏è Data Ingestion:** This is where your existing data from various sources and formats (APIs, PDFs, SQL, etc.) is ingested into the system.\n",
    "\n",
    "**üóÇÔ∏è Data Indexing:** The ingested data is structured into intermediate representations that are easy and performant for Large Language Models (LLMs) to consume.\n",
    "\n",
    "**üêï Retrieval:** Information is retrieved from your data sources based on the question or prompt. This is the first step in the Retrieval-Augmented Generation (RAG) process.\n",
    "\n",
    "**üéñÔ∏è Reranking:** The initially retrieved documents or nodes are reordered based on certain criteria to bring the most relevant or useful nodes to the top.\n",
    "\n",
    "**‰∑æ Post-processing:** After retrieval and reranking, transformations or filters are applied to the set of nodes to further refine them before they are used to generate the final response.\n",
    "\n",
    "**üí¨ Response Generation:** The LLM generates a response based on the enriched prompt, which now includes the context from the retrieved and reranked documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load the data\n",
    "\n",
    "To use data with an LLM, first load it using data connectors, known as `Readers` in LlamaIndex, which format data into `Document` objects containing data and metadata.\n",
    "\n",
    "üìö **SimpleDirectoryReader**:\n",
    "  - The most straightforward loader is `SimpleDirectoryReader``.\n",
    "  - Built into LlamaIndex, it reads various formats (Markdown, PDFs, Word documents, PowerPoint decks, images, audio, video) from every file in a directory, creating documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Transform the data\n",
    "\n",
    "After loading, we must process and transform data for retrieval. We need to transform the list of `Document` objects into `Node` objects \n",
    "\n",
    "- ‚úÇÔ∏è Include chunking, extracting metadata, and embedding each chunk in transformations.\n",
    "\n",
    "- üåü Nodes are a first-class citizen in LlamaIndex, allowing direct definition or parsing from Documents.\n",
    "\n",
    "- üîÑ Transformation inputs and outputs are `Node` objects (Note: `Document` is subclass of `Node`).\n",
    "\n",
    "- üõ†Ô∏è Nodes are \"chunks\" of Documents, including text, images, etc., plus metadata and relationships.\n",
    "\n",
    "- üìä `NodeParser` classes convert Documents into Nodes with all necessary attributes. There are [a number of](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html) `NodeParser`'s you can choose from!\n",
    "\n",
    "- üìë High-level API: Use `.from_documents()` for automatic parsing and chunking of Document objects.\n",
    "\n",
    "- üîç Underlying process splits Document into Node objects, maintaining text and metadata with a link to their parent Document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Indexing\n",
    "\n",
    "An `Index` is a data structure that allows for the quick retrieval of relevant context for a user query. \n",
    "\n",
    "It is the core foundation for retrieval-augmented generation (RAG) use-cases. Indexes are built from `Documents` and are used to build Retrievers, Query Engines and Chat Engines. All of which enable question & answer and chat over your data.\n",
    "\n",
    "- üìÇ After loading your data, you're ready to construct an `Index`.\n",
    "\n",
    "- üåê **Vector Store Index:** The most common Index type. It segments your `Documents` into `Nodes` and generates vector embeddings for each node's text, prepping them for LLM queries.\n",
    "\n",
    "- üîÑ **Vector Store Index Process:** Parse raw texts into document objects, split document objects into chunks/nodes, then convert all your nodes into embeddings and store them in a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Storing\n",
    "\n",
    "Loading and indexing data costs time and money.\n",
    "\n",
    "By default, indexed data is stored in memory. But, you can store your data to avoid the time and costs associated with re-indexing them.  The simplest way to do this **persisting to disk**.\n",
    "\n",
    "Each `Index` object has a `.persist()` method, which will write all the data to disk at the specified location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚òÅÔ∏è Loading in a Vector Database\n",
    "\n",
    "We'll use qdrant as our vector database of choice throughout this course.\n",
    "\n",
    "To use qdrant to store embeddings from the `VectorStoreIndex`, you need to:\n",
    "\n",
    "- Initialize the qdrant client\n",
    "\n",
    "- Create a `Collection` to store your data in qdrant\n",
    "\n",
    "- Assign qdrant as the `vector_store` in a `StorageContext`\n",
    "\n",
    "- Initialize your `VectorStoreIndex` using that `StorageContext`\n",
    "\n",
    "Below, we initialize a `QdrantClient` for interacting with qdrant, an open-source vector store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÉÔ∏è Storage Context\n",
    "\n",
    "`StorageContext` in `LlamaIndex` is a core abstraction that revolves around the storage of `Nodes`, indices, and vectors.  It facilitates data storage and retrieval.\n",
    "\n",
    "It is a utility container that supports the following:\n",
    "\n",
    " - `docstore`: A [`BaseDocumentStore`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/storage/docstore/types.py) for storing nodes.\n",
    "\n",
    " - `index_store`: A [`BaseIndexStore`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/storage/index_store/types.py#L13) for storing indices.\n",
    "\n",
    " - `vector_store`: A [`VectorStore`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/vector_stores/simple.py) for storing vectors.\n",
    "\n",
    " - `graph_store`: A [`GraphStore`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/graph_stores/simple.py) for storing knowledge graphs.\n",
    "\n",
    "Below we instantiate the `StorageContext` from default settings indicating that we want to use a vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü™É Retrieval\n",
    "\n",
    "A `Retriever` is an interface exposed by the `Index`. An `Index` with its `Retriever` is used for storing and fetching data. The `Retriever` is a part of the `Index` and is used to retrieve the data stored in the Index.\n",
    "\n",
    "\n",
    "### LlamaIndex provides [many different types of retrievers](https://github.com/run-llama/llama_index/tree/main/llama-index-core/llama_index/core/retrievers) to fetch relevant information from ingested data based on a given query. \n",
    "\n",
    "Some examples include\n",
    "\n",
    "### Vector Retriever\n",
    "\n",
    "The vector retriever uses vector similarity search to find the most relevant nodes (chunks of text) based on the query embedding. It requires a vector database like to store and search through the node embeddings.\n",
    "\n",
    "### [Fusion Retriever](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/retrievers/fusion_retriever.py)\n",
    "\n",
    "The fusion retriever generates multiple queries from the original query, performs retrieval over an ensemble of retrievers for each query, and then fuses and reranks the results across all queries. This aims to better capture the query intent through query rewriting and ensembling.\n",
    "\n",
    "### [Recursive Retriever](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/retrievers/recursive_retriever.py)\n",
    "\n",
    "The recursive retriever allows for hierarchical retrieval by first retrieving coarse nodes and then recursively retrieving finer-grained nodes within those coarse nodes. This can be useful for multi-level indexing and retrieval.\n",
    "\n",
    "You can also combine retrievers in interesting ways and build out more advanced retrieval strategies, as we will see later in this course.\n",
    "\n",
    "\n",
    "### In the example here, we're using a Vector Retriever\n",
    "\n",
    " - üîç When searching, your query is also converted into a vector embedding. \n",
    " \n",
    "- üóÇÔ∏è The `VectorStoreIndex` then performs a mathematical operation to rank embeddings based on semantic similarity to your query.\n",
    "\n",
    "- üîù Top-k semantic retrieval is the simplest wasy to query a vector index.\n",
    "\n",
    "- ‚©¨ You can also apply a similarity threshold  (e.g., only return results that are more similar than some value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
