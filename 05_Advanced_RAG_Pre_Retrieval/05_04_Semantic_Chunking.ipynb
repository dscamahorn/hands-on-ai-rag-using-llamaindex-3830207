{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#%pip install llama-index llama-index-embeddings-openai llama-index-embeddings-cohere==0.1.9 qdrant-client llama-index-vector-stores-qdrant llama-index-llms-openai llama-index-llms-cohere==0.1.19\n",
    "%pip install llama-index mistralai llama-index-embeddings-mistralai qdrant-client llama-index-vector-stores-qdrant llama-index-llms-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OPENAI_API_KEY key: \")\n",
    "MISTRAL_API_KEY = os.environ['MISTRAL_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"mistral\", \n",
    "    model=\"mistral-small-latest\", \n",
    "    api_key=MISTRAL_API_KEY, \n",
    "    temperature=0.75, \n",
    "    system_prompt=\"\"\"Use ONLY the provided context and generate a complete, coherent answer to the user's query. \n",
    "    Your response must be grounded in the provided context and relevant to the essence of the user's query.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"mistral\",\n",
    "    api_key=MISTRAL_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_documents_from_docstore\n",
    "import random\n",
    "\n",
    "all_documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")\n",
    "\n",
    "random.seed(42)\n",
    "senpai_documents = random.sample(all_documents, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Semantic Chunking\n",
    "\n",
    "This is a recent method that's been popularized by Greg Kamradt. He discussed this in detail in [an informative YouTube video](https://youtu.be/8OJC21T2SL4), which is also a great resource for more information on various chunking strategies.\n",
    "\n",
    "\n",
    "Here's the gist of what semantic chunking does:\n",
    "\n",
    "- Uses sentence embeddings to find breakpoints based on semantic similarity\n",
    "\n",
    "- Keeps related sentences together in the same chunk\n",
    "\n",
    "- Dynamically determines chunk size, no fixed length needed\n",
    "\n",
    "\n",
    "## How semantic chunking works\n",
    "\n",
    "1. ‚úÇÔ∏è Split document into sentences\n",
    "\n",
    "2. üî¢ Index sentences by position\n",
    "\n",
    "3. üéöÔ∏è Choose buffer size (sentences on either side to keep)\n",
    "\n",
    "4. üìä Measure similarity in embedding space\n",
    "   - Keep similar sentences together\n",
    "   - Split dissimilar sentences apart\n",
    "\n",
    "5. üß© Merge groups based on similarity threshold\n",
    "\n",
    "\n",
    "## [`SemanticSplitterNodeParser`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py)\n",
    "\n",
    "SemanticSplitterNodeParser a document into semantically related chunks called nodes. It uses semantic similarity and adaptive breakpoint determination, to create meaningful and coherent nodes from a document. \n",
    "\n",
    "### How it works\n",
    "\n",
    "#### 1. üß© Document Splitting\n",
    "\n",
    "   - The parser takes a document as input.\n",
    "\n",
    "   - It splits the document into individual sentences using a sentence splitter (e.g., split_by_sentence_tokenizer).\n",
    "\n",
    "#### 2. üéöÔ∏è Sentence Grouping\n",
    "\n",
    "   - The parser groups adjacent sentences together based on a configurable buffer size.\n",
    "\n",
    "   - The buffer size determines how many sentences are considered together when evaluating semantic similarity.\n",
    "\n",
    "   - For example, if the buffer size is 1, each sentence is treated individually. If it's greater than 1, sentences are grouped together.\n",
    "\n",
    "#### 3. üåê Embedding Calculation\n",
    "\n",
    "   - The parser calculates embeddings for each group of sentences using an embedding model.\n",
    "\n",
    "   - Embeddings represent the semantic meaning of the sentence groups in a dense vector format.\n",
    "\n",
    "#### 4. üìè Distance Calculation\n",
    "\n",
    "   - The parser calculates the cosine similarity between the embeddings of adjacent sentence groups.\n",
    "\n",
    "   - It then computes the distance by subtracting the similarity from 1.\n",
    "\n",
    "   - These distances represent the semantic dissimilarity between sentence groups.\n",
    "\n",
    "#### 5. üéØ Breakpoint Determination\n",
    "\n",
    "   - The parser determines breakpoints based on a configurable percentile threshold (e.g., 95th percentile).\n",
    "\n",
    "   - If the distance between two adjacent sentence groups exceeds the breakpoint threshold, it indicates a semantic shift and marks the start of a new node.\n",
    "\n",
    "#### 6. üß© Node Creation\n",
    "\n",
    "   - The parser splits the document into nodes based on the determined breakpoints.\n",
    "\n",
    "   - Each node represents a semantically related chunk of text.\n",
    "\n",
    "   - The sentences within a node are combined to form a coherent unit of information.\n",
    "\n",
    "#### 7. üìù Node Metadata\n",
    "\n",
    "   - The parser can include additional metadata in the nodes, such as the original text or other relevant information.\n",
    "\n",
    "   - It can also establish relationships between nodes, such as previous and next relationships, to maintain the sequential order of the chunks.\n",
    "\n",
    "### Arguments you need to know\n",
    "\n",
    "- `embed_model`: The embedding model to use for semantic comparison. If not provided, the parser will attempt to use the OpenAIEmbedding model. If the `llama-index-embeddings-openai llama-index-embeddings-cohere` package is not installed, an ImportError will be raised.\n",
    "\n",
    "- `buffer_size`: The number of sentences to group together when evaluating semantic similarity. \n",
    "\n",
    "  - Default value is 1 (each sentence is considered individually). \n",
    "  \n",
    "  - Increasing the buffer size allows the parser to consider the context of adjacent sentences when determining semantic similarity. This can help capture more meaningful relationships between sentences.\n",
    "\n",
    "- `breakpoint_percentile_threshold`: The percentile of cosine dissimilarity that must be exceeded between a group of sentences and the next to form a node. \n",
    "\n",
    "  - A smaller value results in more nodes being generated. Default value is 95 (95th percentile). \n",
    "\n",
    "  - Adjusting this threshold allows you to control the granularity of the node splits. A lower value will create more nodes, while a higher value will create fewer nodes.\n",
    "\n",
    "- `sentence_splitter`: The function or callable object used to split the text into sentences. Default is `split_by_sentence_tokenizer` (again, using the `PunktSentenceTokenizer` from the `nltk` library). The choice of sentence splitter can affect how the text is divided into individual sentences, which in turn influences the node splitting process.\n",
    "\n",
    "  - While the `sentence_splitter` is used to initially split the document into individual sentences, the actual determination of node boundaries is based on semantic similarity rather than a fixed splitting approach.\n",
    "  \n",
    "  - `sentence_splitter` is more of a preprocessing step to prepare the document for the semantic analysis. It ensures that the parser has a consistent input format to work with (i.e., a list of sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "\n",
    "def semantic_splitter(\n",
    "    embed_model,\n",
    "    buffer_size, \n",
    "    breakpoint_percentile_threshold, \n",
    "    documents,\n",
    "    **kwargs):\n",
    "    splitter = SemanticSplitterNodeParser(\n",
    "        embed_model=embed_model,\n",
    "        buffer_size=buffer_size,\n",
    "        breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n",
    "        )\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You have a lot of design choices to make here\n",
    "\n",
    "These are all points of experimentation for you. Hack around with these, run evaluation against the metrics that matter to you, vibe check the results, and find one that works best.\n",
    "\n",
    "- What embedding model do you want to use?\n",
    "\n",
    "- What's the dimensions you want to use for the embedding model?\n",
    "\n",
    "- What buffer size do you want to use? \n",
    "\n",
    "- What about the breakpoint threshold?\n",
    "\n",
    "- What sentence splitter do you want to use?\n",
    "\n",
    "I'll use some arbitrary settings for illustrative purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_nodes = semantic_splitter(\n",
    "    embed_model = Settings.embed_model,\n",
    "    buffer_size = 3, \n",
    "    breakpoint_percentile_threshold = 5, \n",
    "    documents = senpai_documents\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1348"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(semantic_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': 'b8cc8072-563d-4847-ba0b-b5036fa4f540',\n",
       " 'embedding': None,\n",
       " 'metadata': {'page_number': 98,\n",
       "  'file_name': '../data/hackers_and_painters.pdf',\n",
       "  'title': 'Hackers and Painters',\n",
       "  'author': 'Paul Graham'},\n",
       " 'excluded_embed_metadata_keys': [],\n",
       " 'excluded_llm_metadata_keys': [],\n",
       " 'relationships': {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b61d53d2-3936-4008-a295-646706222485', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_number': 98, 'file_name': '../data/hackers_and_painters.pdf', 'title': 'Hackers and Painters', 'author': 'Paul Graham'}, hash='6affca4b4a44bbe1fbdc2bd6b7dff1c38d239fe3ff8973c1c579ef52c48d671d'),\n",
       "  <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ce42deda-3e7f-406d-a721-b6a4787f8dde', node_type=<ObjectType.TEXT: '1'>, metadata={'page_number': 98, 'file_name': '../data/hackers_and_painters.pdf', 'title': 'Hackers and Painters', 'author': 'Paul Graham'}, hash='05a35e67fac5d43068bda2d2d70cb687c80ca108f7dcfad4ccfcb4612686abc1'),\n",
       "  <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='32b6f497-67a0-4af1-b3e4-b2c46d1aeacd', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4d1cb387b26ce75aa1835803b65dfa714c57cd8b5d4d68b2bfe375fdc4924004')},\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_separator': '\\n',\n",
       " 'text': 'Thats the real point of startups. ',\n",
       " 'mimetype': 'text/plain',\n",
       " 'start_char_idx': 1646,\n",
       " 'end_char_idx': 1680,\n",
       " 'metadata_seperator': '\\n',\n",
       " 'text_template': '{metadata_str}\\n\\n{content}'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_nodes[101].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_number: 98\n",
      "file_name: ../data/hackers_and_painters.pdf\n",
      "title: Hackers and Painters\n",
      "author: Paul Graham\n",
      "\n",
      "Its a much better deal for them to average their work together with a small group of their peers than to average it with everyone.\n"
     ]
    }
   ],
   "source": [
    "print(semantic_nodes[100].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë∑üèΩ‚Äç‚ôÇÔ∏è üóÇÔ∏è Build the Index and Ingest to Qdrant\n",
    "\n",
    "Note: This will also take a long time (about 30 minutes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from utils import create_index, create_query_engine, ingest, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"words-of-the-senpai-semantic-nodes\"\n",
    "\n",
    "semantic_nodes_vector_store = setup_vector_store(\":memory:\", QDRANT_API_KEY, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n"
     ]
    }
   ],
   "source": [
    "transforms = [Settings.embed_model]\n",
    "\n",
    "semantic_nodes = ingest(\n",
    "    documents=semantic_nodes,\n",
    "    transformations=transforms,\n",
    "    vector_store=semantic_nodes_vector_store\n",
    ")\n",
    "\n",
    "semantic_nodes_index = create_index(\n",
    "    from_where=\"vector_store\", \n",
    "    embed_model=Settings.embed_model,\n",
    "    vector_store=semantic_nodes_vector_store\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Setup Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "from utils import create_query_engine\n",
    "from prompts import HYPE_ANSWER_GEN_PROMPT\n",
    "\n",
    "HYPE_ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(HYPE_ANSWER_GEN_PROMPT)\n",
    "\n",
    "semantic_nodes_query_engine = create_query_engine(\n",
    "    index=semantic_nodes_index, \n",
    "    mode=\"query\",\n",
    "    response_mode=\"compact\",\n",
    "    similiarty_top_k=5,\n",
    "    vector_store_query_mode=\"mmr\", \n",
    "    vector_store_kwargs={\"mmr_threshold\": 0.42},\n",
    "    text_qa_template=HYPE_ANSWER_GEN_PROMPT_TEMPLATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Setup Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "semantic_nodes_chain = [input_component,  semantic_nodes_query_engine]\n",
    "\n",
    "semantic_nodes_query_pipeline = create_query_pipeline(semantic_nodes_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 0e63c5bf-5790-4dd6-a671-50be1b343c2b with input: \n",
      "input: How can I navigate the maze of the market while building a company?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 2cb74527-4cfb-447b-b1dc-83db8c8a7afb with input: \n",
      "input: How can I navigate the maze of the market while building a company?\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response=\"Navigate the market maze by mapping out a complex decision tree. Diagram every possible alternative. Explain why your plan is superior to past failures and current competitors. Prove you have a unique idea. Sometimes, pitfalls only appear at scale. If this happens, re-enter the maze with a new strategy. You must be ready to adapt and pivot. This is how you build a company that stands out and wins. Don't be afraid to start over with a new approach if needed. The market is a battlefield, and you must be ready to fight and adapt.\", source_nodes=[NodeWithScore(node=TextNode(id_='cf40983f-9e31-44da-9417-d38b98ee1039', embedding=None, metadata={'page_number': 159, 'file_name': '../data/anthology_of_balaji.pdf', 'title': 'The Anthology of Balaji Srinivasan', 'author': 'Balaji Srinivasan'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='31ccfef4-b875-4712-ba3d-d841c67e0d12', node_type='4', metadata={'page_number': 159, 'file_name': '../data/anthology_of_balaji.pdf', 'title': 'The Anthology of Balaji Srinivasan', 'author': 'Balaji Srinivasan'}, hash='cc867e5fe87fc51187ed9a4d274f2c967e55b98c30464eada133416989fb99a4'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='504be3a7-cd75-469b-882d-4ea0d72b329e', node_type='1', metadata={'page_number': 159, 'file_name': '../data/anthology_of_balaji.pdf', 'title': 'The Anthology of Balaji Srinivasan', 'author': 'Balaji Srinivasan'}, hash='f0f24d8c3cf65766dedb73b1eb1856b1047a19667a8611af789c692c61247737'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3bbbd262-0424-40dd-9f52-97a73cc295e3', node_type='1', metadata={}, hash='a3b90b6645111055bf7c496cbbe83f471a63c8fcc3133d3936531f5dce4309d5')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='If you can write and diagram a complex decision tree with many alternatives, explaining why your particular plan to navigate the maze is superior to the ten past companies who died in the maze and twenty current competitors lost in the maze, you have gone a long way toward proving you have a good idea others did not and do not have. ', mimetype='text/plain', start_char_idx=1116, end_char_idx=1451, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7656119336164603), NodeWithScore(node=TextNode(id_='1ce5bca3-960d-4fcd-bc87-9aac2c7b40e4', embedding=None, metadata={'page_number': 159, 'file_name': '../data/anthology_of_balaji.pdf', 'title': 'The Anthology of Balaji Srinivasan', 'author': 'Balaji Srinivasan'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='31ccfef4-b875-4712-ba3d-d841c67e0d12', node_type='4', metadata={'page_number': 159, 'file_name': '../data/anthology_of_balaji.pdf', 'title': 'The Anthology of Balaji Srinivasan', 'author': 'Balaji Srinivasan'}, hash='cc867e5fe87fc51187ed9a4d274f2c967e55b98c30464eada133416989fb99a4'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='92031266-4961-413b-93e6-c578901f878c', node_type='1', metadata={'page_number': 159, 'file_name': '../data/anthology_of_balaji.pdf', 'title': 'The Anthology of Balaji Srinivasan', 'author': 'Balaji Srinivasan'}, hash='96f3ad5e9d375507950e886984615bec12872c3a1546bb8720434436f0016b07'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3aca6485-9164-45ec-a507-6d9808ab9d96', node_type='1', metadata={}, hash='c8e1edb95d3ea7c18477f54b65c1c11fdb4daa8a46c93033a03239987ce2e806')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Sometimes pitfalls are apparent only when one company reaches scale, and the solution requires re-entering the maze at the very beginning with a new weapon. ', mimetype='text/plain', start_char_idx=374, end_char_idx=531, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7623502901420685)], metadata={'cf40983f-9e31-44da-9417-d38b98ee1039': {'page_number': 159, 'file_name': '../data/anthology_of_balaji.pdf', 'title': 'The Anthology of Balaji Srinivasan', 'author': 'Balaji Srinivasan'}, '1ce5bca3-960d-4fcd-bc87-9aac2c7b40e4': {'page_number': 159, 'file_name': '../data/anthology_of_balaji.pdf', 'title': 'The Anthology of Balaji Srinivasan', 'author': 'Balaji Srinivasan'}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_nodes_query_pipeline.run(input=\"How can I navigate the maze of the market while building a company?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
