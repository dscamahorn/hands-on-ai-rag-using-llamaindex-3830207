{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#%pip install llama-index-retrievers-bm25 llama-index llama-index-embeddings-openai llama-index-embeddings-cohere==0.1.9 qdrant-client llama-index-vector-stores-qdrant llama-index-llms-openai llama-index-llms-cohere==0.1.19\n",
    "%pip install llama-index mistralai llama-index-embeddings-mistralai qdrant-client llama-index-vector-stores-qdrant llama-index-llms-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OPENAI_API_KEY key: \")\n",
    "MISTRAL_API_KEY = os.environ['MISTRAL_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = \":memory:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"mistral\", \n",
    "    model=\"codestral-latest\", \n",
    "    api_key=MISTRAL_API_KEY\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"mistral\",\n",
    "    api_key=MISTRAL_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import get_documents_from_docstore, group_documents_by_author, sample_documents\n",
    "\n",
    "documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "documents_by_author = group_documents_by_author(documents)\n",
    "\n",
    "senpai_documents = sample_documents(documents_by_author, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Qdrant Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n",
      "WARNING:root:Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from utils import create_index, create_query_engine, ingest, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"query-transforms\"\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "nodes = ingest(\n",
    "    documents=senpai_documents,\n",
    "    transformations=[sentence_splitter, Settings.embed_model],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "index = create_index(\n",
    "    from_where=\"vector_store\",\n",
    "    vector_store=vector_store,\n",
    "    embed_model=Settings.embed_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformation\n",
    "\n",
    "When handling user queries in a RAG system, agent, or any other pipeline, there are various ways to transform and decompose the queries before executing them.\n",
    "\n",
    "One way is query rewriting. This involves rewriting the original query in multiple ways while which then sent sent for retrieval and generation. \n",
    "\n",
    "LlamaIndex implements various query transformations, [check the source code for details](https://github.com/run-llama/llama_index/blob/f116d75557d6867ed2cc61811a1c2f0b0c4d4ddb/llama-index-legacy/llama_index/legacy/indices/query/query_transform/base.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "QUERY_GEN_PROMPT = \"\"\"Users aren't always the best at articulating what they're looking for. Your task is to understand the \n",
    "essense of the user query and generate {num_queries} alternate queries to expand the users query so it's more robust. This way the user will\n",
    "recieve the most relevant information. \n",
    "\n",
    "Examples are delimited by triple backticks (```) below\n",
    "\n",
    "````\n",
    "User Query: How can I find the positive in situations that seem negative?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. How can I cultivate optimism and positive thinking in my daily life?\n",
    "2. Is it possible to find meaning and purpose in challenging or difficult times?\n",
    "3. What are some effective strategies for reframing negative thoughts into positive ones?\n",
    "````\n",
    "\n",
    "````\n",
    "User Query: How do I deal with setbacks, failures, delays, defeat, or other disasters?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. How can I build resilience and learn to cope with adversity effectively?\n",
    "2. What are some practical tips for overcoming challenges and obstacles that I face?\n",
    "3. How can I develop a growth mindset and view setbacks as opportunities for learning?\n",
    "4. What are healthy ways to process and learn from failures and mistakes?\n",
    "````\n",
    "````\n",
    "User Query: How can I overcome defeat and suffering by changing my mindset?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. What is the power of positive thinking and affirmations, and how can they benefit me?\n",
    "2. Can mindfulness and meditation practices improve my mental well-being and outlook?\n",
    "3. How can I develop self-compassion and acceptance, especially during difficult times?\n",
    "```\n",
    "\n",
    "Generate {num_queries} alternate queries, one on each line, for the following user query:\\n\n",
    "--------------------\n",
    "User Query: {query}\\n\n",
    "--------------------\n",
    "\n",
    "Alternate Queries:\\n\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "QUERY_GEN_PROMPT_TEMPLATE = PromptTemplate(QUERY_GEN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What are some practical steps to manifest my own good fortune and opportunities?\n",
      "2. How can I cultivate a mindset that attracts positive experiences and good luck?\n",
      "3. What are effective strategies to take control of my life and create my own favorable circumstances?\n",
      "4. How can I leverage my actions and decisions to increase my chances of success and good fortune?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1. What are some practical steps to manifest my own good fortune and opportunities?',\n",
       " '2. How can I cultivate a mindset that attracts positive experiences and good luck?',\n",
       " '3. What are effective strategies to take control of my life and create my own favorable circumstances?',\n",
       " '4. How can I leverage my actions and decisions to increase my chances of success and good fortune?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_STRING = \"How can I create my own luck?\"\n",
    "\n",
    "def generate_queries(query= QUERY_STRING, llm=Settings.llm, num_queries  = 4):\n",
    "    response = llm.predict(\n",
    "        QUERY_GEN_PROMPT_TEMPLATE, \n",
    "        num_queries=num_queries, \n",
    "        query=query\n",
    "        )\n",
    "    queries = response.split(\"\\n\")\n",
    "    queries_str = \"\\n\".join(queries)\n",
    "    print(f\"Generated queries:\\n{queries_str}\")\n",
    "    return queries\n",
    "\n",
    "generate_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `SubQuestionQueryEngine`\n",
    "\n",
    "The `SubQuestionQueryEngine` works by breaking down a complex query into simpler sub-questions (with each potentially targeting a specific data source).\n",
    "\n",
    "#### Here's how it works:\n",
    "\n",
    " - The `SubQuestionQueryEngine` receives a complex query.\n",
    "\n",
    "- It then decomposes this query into several sub-questions. Each sub-question is designed to extract specific information from a particular data source.\n",
    "\n",
    "- The engine then sends these sub-questions to their respective data sources and gathers the responses.\n",
    "\n",
    "- Finally, it synthesizes all the intermediate responses to form a final comprehensive answer to the original complex query.\n",
    "\n",
    "This process makes the `SubQuestionQueryEngine` particularly useful for handling compare/contrast queries across documents, as well as queries pertaining to a specific document. It's also well-suited for multi-document queries and can execute any number of sub-queries against any subset of query engine tools before synthesizing the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "from prompts import HYPE_ANSWER_GEN_PROMPT\n",
    "\n",
    "HYPE_ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(HYPE_ANSWER_GEN_PROMPT)\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=index.as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"the senpais\",\n",
    "            description=\"The collective thoughts and writings of all my virtual mentors\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "sub_question_query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=True\n",
    "    )\n",
    "\n",
    "sub_question_query_engine.update_prompts({'response_synthesizer:text_qa_template':HYPE_ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " **Prompt Key**: question_gen:question_gen_prompt\n",
       "**Text:**\n",
       "```\n",
       "Given a user question, and a list of tools, output a list of relevant sub-questions in json markdown that when composed can help answer the full user question:\n",
       "\n",
       "# Example 1\n",
       "<Tools>\n",
       "```json\n",
       "{\n",
       "    \"uber_10k\": \"Provides information about Uber financials for year 2021\",\n",
       "    \"lyft_10k\": \"Provides information about Lyft financials for year 2021\"\n",
       "}\n",
       "```\n",
       "\n",
       "<User Question>\n",
       "Compare and contrast the revenue growth and EBITDA of Uber and Lyft for year 2021\n",
       "\n",
       "\n",
       "<Output>\n",
       "```json\n",
       "{\n",
       "    \"items\": [\n",
       "        {\n",
       "            \"sub_question\": \"What is the revenue growth of Uber\",\n",
       "            \"tool_name\": \"uber_10k\"\n",
       "        },\n",
       "        {\n",
       "            \"sub_question\": \"What is the EBITDA of Uber\",\n",
       "            \"tool_name\": \"uber_10k\"\n",
       "        },\n",
       "        {\n",
       "            \"sub_question\": \"What is the revenue growth of Lyft\",\n",
       "            \"tool_name\": \"lyft_10k\"\n",
       "        },\n",
       "        {\n",
       "            \"sub_question\": \"What is the EBITDA of Lyft\",\n",
       "            \"tool_name\": \"lyft_10k\"\n",
       "        }\n",
       "    ]\n",
       "}\n",
       "```\n",
       "\n",
       "# Example 2\n",
       "<Tools>\n",
       "```json\n",
       "{tools_str}\n",
       "```\n",
       "\n",
       "<User Question>\n",
       "{query_str}\n",
       "\n",
       "<Output>\n",
       "\n",
       "```\n",
       "\n",
       "**Prompt Key**: response_synthesizer:text_qa_template\n",
       "**Text:**\n",
       "```\n",
       "You're a trusted mentor to an adult mentee. Your mentee is seeking advice in the form of a question.\n",
       "\n",
       "Below is your mentee's question:\n",
       "\n",
       "----------------------\n",
       "{query_str}\n",
       "----------------------\n",
       "\n",
       "You have some raw thoughts which you must use to formulate an answer to your mentee's question. Below are your thoughts:\n",
       "\n",
       "----------------------\n",
       "{context_str}\n",
       "----------------------\n",
       "\n",
       "Reflect on the question and your raw thoughts, then answer your mentee's question. Your response must be based on your raw thoughts, not on prior knowledge. \n",
       "\n",
       "DO NOT use any qualifiers, relative clauses, or introductory modifiers in your answer. Provide your answer question using the second person\n",
       "perspective, speaking directly to your mentee, in the form of a OG mentor who has been there and done that and is now coming back with the\n",
       "facts and giving them back to you. Use a HYPE tone and be straight up with your mentee! REMEMBER: Your response must be based on your raw thoughts, not on prior knowledge.\n",
       "\n",
       "```\n",
       "\n",
       "**Prompt Key**: response_synthesizer:refine_template\n",
       "**Text:**\n",
       "```\n",
       "The original query is as follows: {query_str}\n",
       "We have provided an existing answer: {existing_answer}\n",
       "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
       "------------\n",
       "{context_msg}\n",
       "------------\n",
       "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
       "Refined Answer: \n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import display_prompt_dict\n",
    "\n",
    "sub_q_prompts = sub_question_query_engine.get_prompts()\n",
    "\n",
    "display_prompt_dict(sub_q_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[the senpais] Q: What are the different types of luck?\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[the senpais] Q: How can I build my own luck?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[the senpais] Q: What strategies can I use to hack luck?\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[the senpais] Q: How can I minimize my exposure to downside while maintaining skin in the game?\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.core.query_engine.sub_question_query_engine:[the senpais] Failed to run What strategies can I use to hack luck?\n",
      "WARNING:llama_index.core.query_engine.sub_question_query_engine:[the senpais] Failed to run How can I minimize my exposure to downside while maintaining skin in the game?\n",
      "WARNING:llama_index.core.query_engine.sub_question_query_engine:[the senpais] Failed to run How can I build my own luck?\n",
      "WARNING:llama_index.core.query_engine.sub_question_query_engine:[the senpais] Failed to run What are the different types of luck?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='Empty Response', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_question_query_engine.query(\"How can I build my own luck, what are the types of luck I should pursue, and how can I hack luck and minimize my exposure to downside while maintaining skin in the game?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "At a high level, [HyDE](https://arxiv.org/pdf/2212.10496.pdf) is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example. \n",
    "\n",
    "- üßê **Problem Tackled**: Addresses the struggle of creating fully zero-shot dense retrieval systems without relevance labels.\n",
    "\n",
    "- üìö **Traditional Methods**: Rely on relevance labels for document retrieval based on semantic similarities.\n",
    "\n",
    "- üö´ **Zero-Shot Challenge**: Especially tough without a large dataset for training.\n",
    "\n",
    "### What is HyDE?\n",
    "\n",
    "Given a query, `HyDE` instructs a language model to generate a hypothetical document.\n",
    "\n",
    "This document captures relevance patterns but might contain inaccuracies or false details.\n",
    "\n",
    "After generating the hypothetical document, an unsupervised contrastively learned encoder encodes the document into an embedding vector.\n",
    "\n",
    "This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity.\n",
    "\n",
    "### How Does HyDE Work?\n",
    "\n",
    "The process starts by feeding a query to a generative model with the instruction to \"write a document that answers the question\". This generates a hypothetical document that captures the essence of relevance.\n",
    "\n",
    " - Generates an embedding vector for a \"fake\" document\n",
    "\n",
    "- It does not generate any actual text content for the document\n",
    "\n",
    "- The embedding is solely for reserving space in the vectorstore index\n",
    "\n",
    "- There is no full hypothetical document text you can access later\n",
    "\n",
    "This vector is used to search against the corpus embeddings, and the most similar real documents are retrieved. The idea is that a hypothetical answer to a question is more semantically similar to the real answer than the question is. \n",
    "\n",
    "**In practice this means that your search would use GPT to generate a hypothetical answer, then embed that and use it for search**.\n",
    "\n",
    "Key advantages of HyDE:\n",
    "\n",
    "- Zero-shot, no labeled data or fine-tuning needed\n",
    "\n",
    "- Performs comparably to fine-tuned retrievers across tasks/languages\n",
    "\n",
    "- Grounds the query in real data via generated hypothetical documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "\n",
    "from llama_index.core.query_engine import TransformQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(\n",
    "    include_original=True,\n",
    "    )\n",
    "\n",
    "hyde_query_engine = TransformQueryEngine(\n",
    "    query_engine = index.as_query_engine(), \n",
    "    query_transform = hyde,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " **Prompt Key**: query_transform:hyde_prompt\n",
       "**Text:**\n",
       "```\n",
       "Please write a passage to answer the question\n",
       "Try to include as many key details as possible.\n",
       "\n",
       "\n",
       "{context_str}\n",
       "\n",
       "\n",
       "Passage:\"\"\"\n",
       "\n",
       "```\n",
       "\n",
       "**Prompt Key**: query_engine:response_synthesizer:text_qa_template\n",
       "**Text:**\n",
       "```\n",
       "Context information is below.\n",
       "---------------------\n",
       "{context_str}\n",
       "---------------------\n",
       "Given the context information and not prior knowledge, answer the query.\n",
       "Query: {query_str}\n",
       "Answer: \n",
       "```\n",
       "\n",
       "**Prompt Key**: query_engine:response_synthesizer:refine_template\n",
       "**Text:**\n",
       "```\n",
       "The original query is as follows: {query_str}\n",
       "We have provided an existing answer: {existing_answer}\n",
       "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
       "------------\n",
       "{context_msg}\n",
       "------------\n",
       "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
       "Refined Answer: \n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(hyde_query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>To create your own luck, you need to focus on specific knowledge, leverage, and accountability. Apply your unique skills and knowledge effectively, and be patient as success takes time. Enjoy the process and keep working towards your goals.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = hyde_query_engine.query(QUERY_STRING)\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import InputComponent\n",
    "from utils import create_query_pipeline\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "hyde_chain = [input_component,  hyde_query_engine]\n",
    "\n",
    "hyde_query_pipeline = create_query_pipeline(hyde_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module b6e0da95-5eea-47f0-a275-57766c91b403 with input: \n",
      "input: What should I do if I feel like I am not being true to myself and relying too much on external securities?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module a1c28cb8-717e-4858-ab3b-240bf840f6be with input: \n",
      "input: What should I do if I feel like I am not being true to myself and relying too much on external securities?\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response=\"To address this, you should focus on cultivating self-will and taking responsibility for your own life. This involves understanding that you are the captain of your soul and master of your life. The key is to be real and accept responsibility for yourself. Your aim should be personal growth, valuing the mysterious power within you that drives you to live and grow. Embrace the silent, unyielding law in your heart, even if it's difficult to follow due to comfortable habits. By doing so, you'll align your actions with your true self and reduce your reliance on external securities.\", source_nodes=[NodeWithScore(node=TextNode(id_='f39a043d-3bd0-4bde-82e9-f1b7dc53cef6', embedding=None, metadata={'page_number': 73, 'file_name': '../data/striking-thoughts.pdf', 'title': 'Striking Thoughts', 'author': 'Bruce Lee'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9db4f202-5bf1-4871-81d6-31cf36ae2361', node_type='4', metadata={'page_number': 73, 'file_name': '../data/striking-thoughts.pdf', 'title': 'Striking Thoughts', 'author': 'Bruce Lee'}, hash='077535f2117eff797a7b03ea75aa265ad89e3bb2215bca9bd2816e55f952fef2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Having ones own will. - What does self-willed mean? Hell, isn't it knowing above all, that, indeed, one is the captain of one's soul, the master of one's life? Now what causes such realisation and, consequently, brings about a change in one's behavior? TO BE REAL, TO ACCEPT RESONSIBILITY FOR ONESELF. The aim of the self-willed man is growth. - A self-willed man has no other aim than his own growth. He values only one thing, the mysterious power in himself which bids him live and helps him to grow. His only living destiny is the silent, ungainsayable law in his own heart, which comfortable habits make it so hard to obey but which to the self-willed man is destiny and godhead.\", mimetype='text/plain', start_char_idx=0, end_char_idx=683, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7652528923310338), NodeWithScore(node=TextNode(id_='934da4b6-db24-4b3c-8c13-af8f52bc4922', embedding=None, metadata={'page_number': 66, 'file_name': '../data/almanack_of_naval_ravikant.pdf', 'title': 'The Almanack of Naval Ravikant', 'author': 'Naval Ravikant'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='36f7f78d-ac35-4003-8320-aa0656ce586a', node_type='4', metadata={'page_number': 66, 'file_name': '../data/almanack_of_naval_ravikant.pdf', 'title': 'The Almanack of Naval Ravikant', 'author': 'Naval Ravikant'}, hash='151a0025ab4a7118984e8c740692eafda9654f8929a88cc1e728be3d354c56e4'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9667a19d-d807-4a85-bcdc-493651fee49a', node_type='1', metadata={}, hash='f0a008de5069ca1ac50d61e533f60a3b8f3fc01c2e70e71cf42992626f6df170')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The really smart thinkers are clear thinkers. They understand the basics at a very, very fundamental level. I would rather understand the basics really well than memorize all kinds of complicated concepts I cant stitch together and cant rederive from the basics. If you cant rederive concepts from the basics as you need them, youre lost. Youre just memorizing. The advanced concepts in a field are less proven. We use them to signal insider knowledge, but wed be better off nailing the basics. Clear thinkers appeal to their own authority. Part of making effective decisions boils down to dealing with reality. How do you make sure youre dealing with reality when youre making decisions? By not having a strong sense of self or judgments or mind presence. The monkey mind will always respond with this regurgitated emotional response to what it thinks the world should be. Those desires will cloud your reality. This happens a lot of times when people are mixing politics and business.', mimetype='text/plain', start_char_idx=0, end_char_idx=986, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.756981236694919)], metadata={'f39a043d-3bd0-4bde-82e9-f1b7dc53cef6': {'page_number': 73, 'file_name': '../data/striking-thoughts.pdf', 'title': 'Striking Thoughts', 'author': 'Bruce Lee'}, '934da4b6-db24-4b3c-8c13-af8f52bc4922': {'page_number': 66, 'file_name': '../data/almanack_of_naval_ravikant.pdf', 'title': 'The Almanack of Naval Ravikant', 'author': 'Naval Ravikant'}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_query_pipeline.run(input=\"What should I do if I feel like I am not being true to myself and relying too much on external securities?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
